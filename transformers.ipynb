{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7d2897",
   "metadata": {},
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "In this notebook, we will explore how transformer models (like GPT-2) can generate text based on a given prompt. We will experiment with generating text by adjusting parameters like temperature and sequence length.\n",
    "\n",
    "## Instructions\n",
    "1. Change the prompt below to experiment with different types of text generation.\n",
    "2. Adjust the `max_length` and `temperature` parameters to see how they affect the output.\n",
    "3. Generate at least 3 samples with different prompts and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbce095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 text generation model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Set your prompt\n",
    "prompt = 'In the future, education will'\n",
    "\n",
    "# Generate text\n",
    "\n",
    "# max_length: The maximum length of the generated text.\n",
    "# temperature: Controls the randomness of predictions by scaling the logits before applying softmax.\n",
    "result = generator(prompt, max_length=50, temperature=0.7, truncation=True, pad_token_id=50256)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking news: Artificial intelligence has surpassed human intelligence in medical diagnostics. Experts say our brains have more ability to comprehend complex mental maps and other details than are currently measured by smartphones or the internet, which is surprising because it will make AI work as a self-driving car would today.[1]\n",
      "…but we don't know how much that stuff can handle (or whether you'll ever notice). Google's Wayland project already shows artificial neural networks breaking out of gray matter into highly specific locations around nearby buildings at varying speeds — if those regions contain information about cars running errands\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different prompts\n",
    "\n",
    "# Experiment with News Headlines\n",
    "prompt = \"Breaking news: Artificial intelligence has surpassed human intelligence in medical diagnostics. Experts say\"\n",
    "result = generator(prompt, max_new_tokens=100, temperature=0.9, repetition_penalty=1.2)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "# Experiment with Storytelling\n",
    "prompt = \"Once upon a time, in a world ruled by robots, a young boy discovered\"\n",
    "result = generator(prompt, max_new_tokens=100, temperature=0.8, top_k=50)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "# Experiment with Dialogue\n",
    "prompt = \"Person A: What do you think about the future of technology?\\nPerson B:\"\n",
    "result = generator(prompt, max_length=60, temperature=1.0)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "# Experiment with Questions\n",
    "prompt = \"What are the implications of quantum computing on society?\"\n",
    "result = generator(prompt, max_length=70, temperature=0.5)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "# Experiment with Multiple Outputs\n",
    "prompt = 'The future of transportation includes'\n",
    "result = generator(prompt, max_length=60, temperature=0.9, num_return_sequences=3)\n",
    "\n",
    "for i, r in enumerate(result):\n",
    "    print(f\"\\nSample {i+1}:\\n{r['generated_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d0d32",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Now that you have experimented with text generation, write a brief report on your observations.\n",
    "\n",
    "1. What patterns did you notice in the generated text?\n",
    "2. How did changing the temperature affect the creativity and coherence of the text?\n",
    "3. What types of prompts yielded the most coherent results?\n",
    "4. What are the limitations of GPT-2 based on your experimentation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
